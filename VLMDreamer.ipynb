{"cells":[{"cell_type":"markdown","metadata":{"id":"yDN8Ohc0Mh8C"},"source":["# 環境構築"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yePZImjczzNU"},"outputs":[],"source":["!git clone https://github.com/AlignmentResearch/vlmrm.git"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"id":"k-Tns5JvffDZ"},"outputs":[],"source":["!tar -zxvf mujoco210-linux-x86_64.tar.gz\n","!mkdir ~/.mujoco\n","!cp -r mujoco210 ~/.mujoco/mujoco210\n","!cp -r ~/.mujoco/mujoco210/bin/* /usr/lib/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzBnWZE_ffDZ"},"outputs":[],"source":["%cd vlmrm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49MbwGCWYpHB","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n","\n","!pip install -e \".[dev]\"\n","\n","!echo y | pip uninstall opencv-contrib-python=='4.8.0.76'\n","!pip install opencv-contrib-python=='4.8.0.74'\n","!apt update && apt install libosmesa6-dev libgl1-mesa-glx libglfw3-dev patchelf xvfb freeglut3-dev libgles2-mesa-dev -y\n","!pip3 install -U 'mujoco-py<2.2,>=2.1' pyvirtualdisplay"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIZrOaUWb6J_"},"outputs":[],"source":["%cd 'src'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC1VmoSHffDc"},"outputs":[],"source":["import os\n","os.environ[\"LD_LIBRARY_PATH\"] = \"/root/.mujoco/mujoco210/bin\"\n","os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n","os.environ[\"MUJOCO_GL\"] = \"osmesa\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72zXgw6dMZ3_"},"outputs":[],"source":["import time\n","import gc\n","import random\n","import pathlib\n","from typing import Any, Dict, List, Optional, Tuple, overload\n","from numpy.typing import NDArray\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import open_clip\n","\n","import mujoco_py\n","import gymnasium\n","from gymnasium import utils\n","from gymnasium.envs.mujoco import MujocoEnv\n","from gymnasium.envs.mujoco.humanoid_v4 import HumanoidEnv as GymHumanoidEnv\n","from gymnasium.spaces import Box\n","\n","import torch\n","from torch.distributions import Normal\n","from torch.distributions.kl import kl_divergence\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.nn.utils import clip_grad_norm_\n","\n","\n","from vlmrm.contrib.open_clip.transform import image_transform\n","from vlmrm.trainer.config import CLIPRewardConfig, Config\n","\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EApZNxvffDd"},"outputs":[],"source":["from pyvirtualdisplay import Display\n","pydisplay = Display(visible=0, size=(400, 300))\n","pydisplay.start()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pKzkQ_Jopg_"},"outputs":[],"source":["env = gymnasium.make(\"Humanoid-v4\", render_mode=\"rgb_array\")\n","env.reset()\n","image = env.render()\n","plt.imshow(image) # (480, 480, 3)\n","plt.show()\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"842Ygf8_nhW1"},"source":["# Reward Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-rhUO_cfvZi"},"outputs":[],"source":["vlm = {\"env_name\": \"Humanoid-v4\",\n","       \"base_path\": \"data/runs/training\",\n","       \"seed\": 42,\n","       \"description\": \"Humanoid kneeling\",\n","       \"tags\": [\"kneeling\", \"humanoid\", \"clip\", \"model-scaling\"],\n","       \"reward\": {\n","                  \"name\": \"clip\",\n","                  \"pretrained_model\": \"ViT-bigG-14/laion2b_s39b_b160k\",\n","                  \"batch_size\": 1,\n","                  \"alpha\": 0.0,\n","                  \"target_prompts\": [\"a humanoid robot kneeling\"],\n","                  \"baseline_prompts\": [\"a humanoid robot\"],\n","                  \"cache_dir\": \"/data/cache\",\n","                  \"camera_config\": {\n","                      \"trackbodyid\": 1,\n","                      \"distance\": 3.5,\n","                      \"lookat\": [0.0, 0.0, 1.0],\n","                      \"elevation\": -10.0,\n","                      \"azimuth\": 180.0,\n","                  }\n","       },\n","       \"rl\": {\"policy_name\": \"MlpPolicy\",\n","            \"n_steps\": 10000000,\n","            \"n_envs_per_worker\": 8,\n","            \"episode_length\": 100,\n","            \"learning_starts\": 50000,\n","            \"train_freq\": 100,\n","            \"batch_size\": 512,\n","            \"gradient_steps\": 100,\n","            \"tau\": 0.005,\n","            \"gamma\": 0.95,\n","            \"learning_rate\": 6e-4\n","        },\n","        \"logging\": {\n","                  \"checkpoint_freq\": 128000,\n","                  \"video_freq\": 128000\n","                }\n","       }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdVDyl7Bvppm"},"outputs":[],"source":["config = Config(**vlm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xL77kHQhE2Uq"},"outputs":[],"source":["# src/vlmrm/envs/mujoco/clip_rewarded_humanoid.py\n","\n","DEFAULT_CAMERA_CONFIG = {\n","    \"trackbodyid\": 1,\n","    \"distance\": 3.5,\n","    \"lookat\": np.array((0.0, 0.0, 1.0)),\n","    \"elevation\": -10.0,\n","    \"azimuth\": 180.0,\n","}\n","\n","class CLIPRewardedHumanoidEnv(GymHumanoidEnv):\n","    def __init__(\n","        self,\n","        episode_length: int = 100,\n","        render_mode: str = \"rgb_array\",\n","        forward_reward_weight: float = 1.25,\n","        ctrl_cost_weight: float = 0.1,\n","        healthy_reward: float = 5.0,\n","        healthy_z_range: Tuple[float] = (1.0, 2.0),\n","        reset_noise_scale: float = 1e-2,\n","        exclude_current_positions_from_observation: bool = True,\n","        camera_config: Optional[Dict[str, Any]] = DEFAULT_CAMERA_CONFIG,\n","        textured: bool = True,\n","        **kwargs,\n","    ):\n","        terminate_when_unhealthy = False\n","        utils.EzPickle.__init__(\n","            self,\n","            forward_reward_weight,\n","            ctrl_cost_weight,\n","            healthy_reward,\n","            terminate_when_unhealthy,\n","            healthy_z_range,\n","            reset_noise_scale,\n","            exclude_current_positions_from_observation,\n","            render_mode=render_mode,\n","            **kwargs,\n","        )\n","\n","        self._forward_reward_weight = forward_reward_weight\n","        self._ctrl_cost_weight = ctrl_cost_weight\n","        self._healthy_reward = healthy_reward\n","        self._terminate_when_unhealthy = terminate_when_unhealthy\n","        self._healthy_z_range = healthy_z_range\n","\n","        self._reset_noise_scale = reset_noise_scale\n","\n","        self._exclude_current_positions_from_observation = (\n","            exclude_current_positions_from_observation\n","        )\n","\n","        if exclude_current_positions_from_observation:\n","            observation_space = Box(\n","                low=-np.inf, high=np.inf, shape=(64, 64, 3), dtype=np.float64\n","            )\n","        else:\n","            observation_space = Box(\n","                low=-np.inf, high=np.inf, shape=(378,), dtype=np.float64\n","            )\n","        env_file_name = None\n","        if textured:\n","            env_file_name = \"humanoid_textured.xml\"\n","        else:\n","            env_file_name = \"humanoid.xml\"\n","        model_path = 'vlmrm/envs/mujoco/' + env_file_name\n","        MujocoEnv.__init__(\n","            self,\n","            model_path,\n","            5,\n","            observation_space=observation_space,\n","            width=64,\n","            height=64,\n","            default_camera_config=camera_config,\n","            render_mode=render_mode,\n","            **kwargs,\n","        )\n","        self.episode_length = episode_length\n","        self.num_steps = 0\n","        if camera_config:\n","            self.camera_id = -1\n","\n","    def step(self, action) -> Tuple[NDArray, float, bool, bool, Dict]:\n","        obs, reward, terminated, truncated, info = super().step(action)\n","        obs = self.render()\n","        self.num_steps += 1\n","        terminated = self.num_steps >= self.episode_length\n","        return obs, reward, terminated, info\n","\n","    def reset(self, *, seed: Optional[int] = 42, options: Optional[Dict] = None):\n","        self.num_steps = 0\n","        obs = self.render()\n","        return obs  # super().reset(seed=seed, options=options)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBMciu83xSBt"},"outputs":[],"source":["# src/vlmrm/reward_model.py\n","\n","class CLIPEmbed(nn.Module):\n","    def __init__(self, clip_model):\n","        super().__init__()\n","        self.clip_model = clip_model\n","        if isinstance(clip_model.visual.image_size, int):\n","            image_size = clip_model.visual.image_size\n","        else:\n","            image_size = clip_model.visual.image_size[0]\n","        self.transform = image_transform(image_size)\n","\n","    @torch.inference_mode()\n","    def forward(self, x):\n","        if x.shape[1] != 3:\n","            x = x.permute(0, 3, 1, 2)\n","\n","        with torch.no_grad(), torch.autocast(\"cuda\", enabled=torch.cuda.is_available()):\n","            x = self.transform(x)\n","            x = self.clip_model.encode_image(x, normalize=True)\n","        return x\n","\n","\n","class CLIPReward(nn.Module):\n","    def __init__(\n","        self,\n","        *,\n","        model: CLIPEmbed,\n","        alpha: float,\n","        target_prompts: torch.Tensor,\n","        baseline_prompts: torch.Tensor,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            model (str): CLIP model.\n","            device (str): Device to use.\n","            alpha (float, optional): Coefficient of projection.\n","            target_prompts (torch.Tensor): Tokenized prompts describing\n","                the target state.\n","            baseline_prompts (torch.Tensor): Tokenized prompts describing\n","                the baseline state.\n","        \"\"\"\n","        super().__init__()\n","        self.embed_module = model\n","        target = self.embed_prompts(target_prompts).mean(dim=0, keepdim=True)\n","        baseline = self.embed_prompts(baseline_prompts).mean(dim=0, keepdim=True)\n","        direction = target - baseline\n","        # Register them as buffers so they are automatically moved around.\n","        self.register_buffer(\"target\", target)\n","        self.register_buffer(\"baseline\", baseline)\n","        self.register_buffer(\"direction\", direction)\n","\n","        self.alpha = alpha\n","        projection = self.compute_projection(alpha)\n","        self.register_buffer(\"projection\", projection)\n","\n","    def compute_projection(self, alpha: float) -> torch.Tensor:\n","        projection = self.direction.T @ self.direction / torch.norm(self.direction) ** 2\n","        identity = torch.diag(torch.ones(projection.shape[0])).to(projection.device)\n","        projection = alpha * projection + (1 - alpha) * identity\n","        return projection\n","\n","    def update_alpha(self, alpha: float) -> None:\n","        self.alpha = alpha\n","        self.projection = self.compute_projection(alpha)\n","\n","    @torch.inference_mode()\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x / torch.norm(x, dim=-1, keepdim=True)\n","        y = 1 - (torch.norm((x - self.target) @ self.projection, dim=-1) ** 2) / 2\n","        return y\n","\n","    @staticmethod\n","    def tokenize_prompts(x: List[str]) -> torch.Tensor:\n","        \"\"\"Tokenize a list of prompts.\"\"\"\n","        return open_clip.tokenize(x)\n","\n","    def embed_prompts(self, x) -> torch.Tensor:\n","        \"\"\"Embed a list of prompts.\"\"\"\n","        with torch.no_grad():\n","            x = self.embed_module.clip_model.encode_text(x).float()\n","        x = x / x.norm(dim=-1, keepdim=True)\n","        return x\n","\n","    def embed_images(self, x):\n","        return self.embed_module.forward(x)\n","\n","\n","def load_reward_model(model_name, target_prompts, baseline_prompts, alpha, cache_dir: Optional[str] = None):\n","    model_name_prefix, pretrained = model_name.split(\"/\")\n","    model = open_clip.create_model(\n","        model_name=model_name_prefix, pretrained=pretrained, cache_dir=cache_dir\n","    )\n","    target_prompts = CLIPReward.tokenize_prompts(target_prompts)\n","    baseline_prompts = CLIPReward.tokenize_prompts(baseline_prompts)\n","    model = CLIPEmbed(model)\n","    model = CLIPReward(\n","        model=model,\n","        alpha=alpha,\n","        target_prompts=target_prompts,\n","        baseline_prompts=baseline_prompts,\n","    )\n","    return model.eval()\n","\n","\n","def load_reward_model_from_config(config: CLIPRewardConfig) -> CLIPReward:\n","    return load_reward_model(\n","        model_name=config.pretrained_model,\n","        target_prompts=config.target_prompts,\n","        baseline_prompts=config.baseline_prompts,\n","        alpha=config.alpha,\n","        cache_dir=config.cache_dir,\n","    )\n","\n","\n","def compute_reward(model: CLIPEmbed, frame: torch.Tensor) -> torch.Tensor:\n","    reward_model = model.eval()\n","    with torch.no_grad():\n","        embedding = reward_model.embed_module(frame)\n","        reward = reward_model(embedding)\n","\n","    reward = reward.to('cpu').detach().numpy()\n","    return reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mtu1t8PvrV_h"},"outputs":[],"source":["# src/vlmrm/contrib/sb3/clip_rewarded_sac.py\n","\n","device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n","reward_model = load_reward_model_from_config(config.reward).to(device)\n","\n","def clip_reward(obs: NDArray, model: CLIPReward) -> NDArray:\n","    frame = obs.copy()\n","    frame = torch.from_numpy(frame)\n","    frame = torch.unsqueeze(frame, dim=0)\n","    frame = frame.permute(0, 3, 1, 2)\n","    upsample = nn.Upsample(scale_factor=3.5, mode='bilinear')\n","    frame = upsample(frame)\n","\n","    reward = compute_reward(\n","        model=model,\n","        frame=frame.to(device),\n","    )\n","    return reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-teiYkd6dmr"},"outputs":[],"source":["class RepeatAction(gymnasium.Wrapper):\n","    def __init__(self, env, skip=2, model=reward_model):\n","        gymnasium.Wrapper.__init__(self, env)\n","        self._skip = skip\n","        self.reward_model = reward_model\n","\n","    def reset(self):\n","        return self.env.reset()\n","\n","    def step(self, action):\n","        total_reward = 0.0\n","        for _ in range(self._skip):\n","            obs, reward, done, info = self.env.step(action)\n","            reward = clip_reward(obs, self.reward_model)\n","            total_reward += reward\n","\n","            if done:\n","                break\n","        return obs, total_reward, done, info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QBzR-DK9HOQ"},"outputs":[],"source":["def humanoid_env(seed=42):\n","    env = CLIPRewardedHumanoidEnv()\n","\n","    env.action_space.seed(seed)\n","    env.observation_space.seed(seed)\n","\n","    env = RepeatAction(env)\n","\n","    return env"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwFmpA5qSb8I"},"outputs":[],"source":["def set_seed(seed: int) -> None:\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"61uC5GNog_Iq"},"outputs":[],"source":["env = humanoid_env()\n","env.reset()\n","obs, reward, terminated, info = env.step(env.action_space.sample())\n","plt.imshow(obs) # (64, 64, 3)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"B3rWEGtkA3e_"},"source":["# モデルの実装\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUt9Lp3HMeJq"},"outputs":[],"source":["class TransitionModel(nn.Module):\n","    def __init__(self, state_dim, action_dim, rnn_hidden_dim,\n","                 hidden_dim=200, min_stddev=0.1, act=F.elu):\n","        super(TransitionModel, self).__init__()\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.fc_state_action = nn.Linear(state_dim + action_dim, hidden_dim)\n","\n","        self.fc_rnn_hidden = nn.Linear(rnn_hidden_dim, hidden_dim)\n","        self.fc_state_mean_prior = nn.Linear(hidden_dim, state_dim)\n","        self.fc_state_stddev_prior = nn.Linear(hidden_dim, state_dim)\n","        self.fc_rnn_hidden_embedded_obs = nn.Linear(rnn_hidden_dim + 1024, hidden_dim)\n","        self.fc_state_mean_posterior = nn.Linear(hidden_dim, state_dim)\n","        self.fc_state_stddev_posterior = nn.Linear(hidden_dim, state_dim)\n","\n","        self.rnn = nn.GRUCell(hidden_dim, rnn_hidden_dim)\n","        self._min_stddev = min_stddev\n","        self.act = act\n","\n","\n","    def forward(self, state, action, rnn_hidden, embedded_next_obs):\n","        next_state_prior, rnn_hidden = self.prior(self.recurrent(state, action, rnn_hidden))\n","        next_state_posterior = self.posterior(rnn_hidden, embedded_next_obs)\n","        return next_state_prior, next_state_posterior, rnn_hidden\n","\n","    def recurrent(self, state, action, rnn_hidden):\n","        \"\"\"\n","        h_t+1 = f(h_t, s_t, a_t)\n","        \"\"\"\n","        hidden = self.act(self.fc_state_action(torch.cat([state, action], dim=1)))\n","        rnn_hidden = self.rnn(hidden, rnn_hidden)\n","        return rnn_hidden\n","\n","    def prior(self, rnn_hidden):\n","        \"\"\"\n","        prior p(s_t+1 | h_t+1)\n","        \"\"\"\n","        hidden = self.act(self.fc_rnn_hidden(rnn_hidden))\n","\n","        mean = self.fc_state_mean_prior(hidden)\n","        stddev = F.softplus(self.fc_state_stddev_prior(hidden)) + self._min_stddev\n","        return Normal(mean, stddev), rnn_hidden\n","\n","    def posterior(self, rnn_hidden, embedded_obs):\n","        \"\"\"\n","        posterior q(s_t+1 | h_t+1, e_t+1)\n","        \"\"\"\n","        hidden = self.act(\n","            self.fc_rnn_hidden_embedded_obs(\n","                torch.cat([rnn_hidden, embedded_obs], dim=1)\n","            )\n","        )\n","        mean = self.fc_state_mean_posterior(hidden)\n","        stddev = F.softplus(self.fc_state_stddev_posterior(hidden)) + self._min_stddev\n","        return Normal(mean, stddev)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmsUzBFYMeJq"},"outputs":[],"source":["class ObservationModel(nn.Module):\n","    \"\"\"\n","    p(o_t | s_t, h_t)\n","    \"\"\"\n","    def __init__(self, state_dim, rnn_hidden_dim):\n","        super(ObservationModel, self).__init__()\n","        self.fc = nn.Linear(state_dim + rnn_hidden_dim, 1024)\n","        self.dc1 = nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2)\n","        self.dc2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2)\n","        self.dc3 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2)\n","        self.dc4 = nn.ConvTranspose2d(32, 3, kernel_size=6, stride=2)\n","\n","\n","    def forward(self, state, rnn_hidden):\n","        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n","        hidden = hidden.view(hidden.size(0), 1024, 1, 1)\n","        hidden = F.relu(self.dc1(hidden))\n","        hidden = F.relu(self.dc2(hidden))\n","        hidden = F.relu(self.dc3(hidden))\n","        obs = self.dc4(hidden)\n","        return obs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4c0l4yGSo9Fk"},"outputs":[],"source":["class RewardModel(nn.Module):\n","    \"\"\"\n","    p(r_t | s_t, h_t)\n","    \"\"\"\n","    def __init__(self, state_dim, rnn_hidden_dim, hidden_dim=400, act=F.elu):\n","        super(RewardModel, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","        self.act = act\n","\n","\n","    def forward(self, state, rnn_hidden):\n","        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = self.act(self.fc2(hidden))\n","        hidden = self.act(self.fc3(hidden))\n","        reward = self.fc4(hidden)\n","        return reward"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzJB9ZiOxMnn"},"outputs":[],"source":["class RSSM(nn.Module):\n","    def __init__(self, state_dim, action_dim, rnn_hidden_dim):\n","        super().__init__()\n","\n","        self.transition = TransitionModel(state_dim, action_dim, rnn_hidden_dim).to(device)\n","        self.observation = ObservationModel(state_dim, rnn_hidden_dim,).to(device)\n","        self.reward = RewardModel(state_dim, rnn_hidden_dim,).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tmptCR67aYZc"},"outputs":[],"source":["class ReplayBuffer(object):\n","    def __init__(self, capacity, observation_shape, action_dim):\n","        self.capacity = capacity\n","\n","        self.observations = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n","        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n","        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n","        self.done = np.zeros((capacity, 1), dtype=np.bool_)\n","\n","        self.index = 0\n","        self.is_filled = False\n","\n","    def push(self, observation, action, reward, done):\n","        self.observations[self.index] = observation\n","        self.actions[self.index] = action\n","        self.rewards[self.index] = reward\n","        self.done[self.index] = done\n","\n","        if self.index == self.capacity - 1:\n","            self.is_filled = True\n","        self.index = (self.index + 1) % self.capacity\n","\n","    def sample(self, batch_size, chunk_length):\n","        episode_borders = np.where(self.done)[0]\n","        sampled_indexes = []\n","        for _ in range(batch_size):\n","            cross_border = True\n","            while cross_border:\n","                initial_index = np.random.randint(len(self) - chunk_length + 1)\n","                final_index = initial_index + chunk_length - 1\n","                cross_border = np.logical_and(initial_index <= episode_borders,\n","                                              episode_borders < final_index).any()\n","            sampled_indexes += list(range(initial_index, final_index + 1))\n","\n","        sampled_observations = self.observations[sampled_indexes].reshape(\n","            batch_size, chunk_length, *self.observations.shape[1:])\n","        sampled_actions = self.actions[sampled_indexes].reshape(\n","            batch_size, chunk_length, self.actions.shape[1])\n","        sampled_rewards = self.rewards[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        sampled_done = self.done[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n","\n","    def __len__(self):\n","        return self.capacity if self.is_filled else self.index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLIt0qiya1_2"},"outputs":[],"source":["def preprocess_obs(obs):\n","    \"\"\"\n","    [0, 255] -> [-0.5, 0.5]\n","    \"\"\"\n","    obs = obs.astype(np.float32)\n","    normalized_obs = obs / 255.0 - 0.5\n","    return normalized_obs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLZMKJj_bIHr"},"outputs":[],"source":["def lambda_target(rewards, values, gamma, lambda_):\n","    \"\"\"\n","    λ-return\n","    \"\"\"\n","    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n","\n","    H = rewards.shape[0] - 1\n","    V_n = torch.zeros_like(rewards, device=rewards.device)\n","    V_n[H] = values[H]\n","    for n in range(1, H+1):\n","        V_n[:-n] = (gamma ** n) * values[n:]\n","        for k in range(1, n+1):\n","            if k == n:\n","                V_n[:-n] += (gamma ** (n-1)) * rewards[k:]\n","            else:\n","                V_n[:-n] += (gamma ** (k-1)) * rewards[k:-n+k]\n","\n","        if n == H:\n","            V_lambda += (lambda_ ** (H-1)) * V_n\n","        else:\n","            V_lambda += (1 - lambda_) * (lambda_ ** (n-1)) * V_n\n","\n","    return V_lambda"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGY-YZzmSh-3"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.cv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2)\n","        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.cv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2)\n","        self.cv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2)\n","\n","    def forward(self, obs):\n","        hidden = F.relu(self.cv1(obs))\n","        hidden = F.relu(self.cv2(hidden))\n","        hidden = F.relu(self.cv3(hidden))\n","        embedded_obs = F.relu(self.cv4(hidden)).reshape(hidden.size(0), -1)\n","        return embedded_obs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gegVfLVWLLR"},"outputs":[],"source":["class ValueModel(nn.Module):\n","    def __init__(self, state_dim, rnn_hidden_dim, hidden_dim=400, act=F.elu):\n","        super(ValueModel, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","        self.act = act\n","\n","    def forward(self, state, rnn_hidden):\n","        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = self.act(self.fc2(hidden))\n","        hidden = self.act(self.fc3(hidden))\n","        state_value = self.fc4(hidden)\n","        return state_value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_2iSa_-kW_W7"},"outputs":[],"source":["class ActionModel(nn.Module):\n","    def __init__(self, state_dim, rnn_hidden_dim, action_dim,\n","                 hidden_dim=400, act=F.elu, min_stddev=1e-4, init_stddev=5.0):\n","        super(ActionModel, self).__init__()\n","        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc_mean = nn.Linear(hidden_dim, action_dim)\n","        self.fc_stddev = nn.Linear(hidden_dim, action_dim)\n","        self.act = act\n","        self.min_stddev = min_stddev\n","        self.init_stddev = np.log(np.exp(init_stddev) - 1)\n","\n","    def forward(self, state, rnn_hidden, training=True):\n","        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = self.act(self.fc2(hidden))\n","        hidden = self.act(self.fc3(hidden))\n","        hidden = self.act(self.fc4(hidden))\n","\n","        mean = self.fc_mean(hidden)\n","        mean = 5.0 * torch.tanh(mean / 5.0)\n","        stddev = self.fc_stddev(hidden)\n","        stddev = F.softplus(stddev + self.init_stddev) + self.min_stddev\n","\n","        if training:\n","            action = torch.tanh(Normal(mean, stddev).rsample())\n","        else:\n","            action = torch.tanh(mean)\n","        return action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dof4rokgYj-6"},"outputs":[],"source":["class Agent:\n","    def __init__(self, encoder, rssm, action_model):\n","        self.encoder = encoder\n","        self.rssm = rssm\n","        self.action_model = action_model\n","\n","        self.device = next(self.action_model.parameters()).device\n","        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n","\n","    def __call__(self, obs, training=True):\n","        obs = preprocess_obs(obs)\n","        obs = torch.as_tensor(obs, device=self.device)\n","        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            embedded_obs = self.encoder(obs)\n","            state_posterior = self.rssm.posterior(self.rnn_hidden, embedded_obs)\n","            state = state_posterior.sample()\n","            action = self.action_model(state, self.rnn_hidden, training=training)\n","            _, self.rnn_hidden = self.rssm.prior(self.rssm.recurrent(state, action, self.rnn_hidden))\n","\n","        return action.squeeze().cpu().numpy()\n","\n","    def reset(self):\n","        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)"]},{"cell_type":"markdown","metadata":{"id":"QUPWAkpyc9Z-"},"source":["# モデルの学習"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8sW8K5Kdjrg"},"outputs":[],"source":["set_seed(42)\n","env = humanoid_env()\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","buffer_capacity = 200000\n","replay_buffer = ReplayBuffer(\n","    capacity=buffer_capacity,\n","    observation_shape=env.observation_space.shape,\n","    action_dim=env.action_space.shape[0]\n",")\n","\n","state_dim = 30\n","rnn_hidden_dim = 200\n","\n","encoder = Encoder().to(device)\n","rssm = RSSM(state_dim,env.action_space.shape[0],rnn_hidden_dim, )\n","value_model = ValueModel(state_dim, rnn_hidden_dim).to(device)\n","action_model = ActionModel(state_dim, rnn_hidden_dim,\n","                             env.action_space.shape[0]).to(device)\n","\n","trained_models = TrainedModels(\n","    encoder, rssm, value_model, action_model\n",")\n","\n","\n","model_lr = 6e-4\n","value_lr = 8e-5\n","action_lr = 8e-5\n","eps = 1e-4\n","model_params = (list(encoder.parameters()) +\n","                  list(rssm.transition.parameters()) +\n","                  list(rssm.observation.parameters()) +\n","                  list(rssm.reward.parameters()))\n","model_optimizer = torch.optim.Adam(model_params, lr=model_lr, eps=eps)\n","value_optimizer = torch.optim.Adam(value_model.parameters(), lr=value_lr, eps=eps)\n","action_optimizer = torch.optim.Adam(action_model.parameters(), lr=action_lr, eps=eps)\n","\n","\n","seed_episodes = 2 # 5\n","all_episodes = 10 # 100\n","test_interval = 2 # 10\n","model_save_interval = 4 # 20\n","collect_interval = 10 # 100\n","\n","action_noise_var = 0.3\n","\n","batch_size = 50\n","chunk_length = 50\n","imagination_horizon = 15\n","\n","gamma = 0.9\n","lambda_ = 0.95\n","clip_grad_norm = 100\n","free_nats = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txKI9CCne5e3"},"outputs":[],"source":["for episode in range(seed_episodes):\n","    obs = env.reset()\n","    done = False\n","    while not done:\n","        action = env.action_space.sample()\n","        next_obs, reward, done, _= env.step(action)\n","        replay_buffer.push(obs, action, reward, done)\n","        obs = next_obs"]},{"cell_type":"code","source":["log_dir = \"logs\"\n","writer = SummaryWriter(log_dir)"],"metadata":{"id":"zOSPRn2djchC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_LNryZQfnKT"},"outputs":[],"source":["for episode in range(seed_episodes, all_episodes):\n","    start = time.time()\n","    policy = Agent(encoder, rssm.transition, action_model)\n","\n","    env = CLIPRewardedHumanoidEnv()\n","    obs = env.reset()\n","    done = False\n","    total_reward = 0\n","    while not done:\n","        action = policy(obs)\n","        action += np.random.normal(0, np.sqrt(action_noise_var),\n","                                     env.action_space.shape[0])\n","        next_obs, reward, done, _, = env.step(action)\n","\n","        replay_buffer.push(obs, action, reward, done)\n","\n","        obs = next_obs\n","        total_reward += reward\n","\n","    print('episode [%4d/%4d] is collected. Total reward is %f' %\n","            (episode+1, all_episodes, total_reward))\n","    print('elasped time for interaction: %.2fs' % (time.time() - start))\n","\n","    start = time.time()\n","    for update_step in range(collect_interval):\n","        observations, actions, rewards, _ = \\\n","            replay_buffer.sample(batch_size, chunk_length)\n","\n","        observations = preprocess_obs(observations)\n","        observations = torch.as_tensor(observations, device=device)\n","        observations = observations.transpose(3, 4).transpose(2, 3)\n","        observations = observations.transpose(0, 1)\n","        actions = torch.as_tensor(actions, device=device).transpose(0, 1)\n","        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)\n","\n","        embedded_observations = encoder(\n","            observations.reshape(-1, 3, 64, 64)).view(chunk_length, batch_size, -1)\n","\n","        states = torch.zeros(chunk_length, batch_size, state_dim, device=device)\n","        rnn_hiddens = torch.zeros(chunk_length, batch_size, rnn_hidden_dim, device=device)\n","\n","        state = torch.zeros(batch_size, state_dim, device=device)\n","        rnn_hidden = torch.zeros(batch_size, rnn_hidden_dim, device=device)\n","\n","        kl_loss = 0\n","        for l in range(chunk_length-1):\n","            next_state_prior, next_state_posterior, rnn_hidden = \\\n","                rssm.transition(state, actions[l], rnn_hidden, embedded_observations[l+1])\n","            state = next_state_posterior.rsample()\n","            states[l+1] = state\n","            rnn_hiddens[l+1] = rnn_hidden\n","            kl = kl_divergence(next_state_prior, next_state_posterior).sum(dim=1)\n","            kl_loss += kl.clamp(min=free_nats).mean()\n","        kl_loss /= (chunk_length - 1)\n","\n","        states = states[1:]\n","        rnn_hiddens = rnn_hiddens[1:]\n","\n","        flatten_states = states.view(-1, state_dim)\n","        flatten_rnn_hiddens = rnn_hiddens.view(-1, rnn_hidden_dim)\n","        recon_observations = rssm.observation(flatten_states, flatten_rnn_hiddens).view(chunk_length-1, batch_size, 3, 64, 64)\n","        predicted_rewards = rssm.reward(flatten_states, flatten_rnn_hiddens).view(chunk_length-1, batch_size, 1)\n","\n","        obs_loss = 0.5 * F.mse_loss(recon_observations, observations[1:], reduction='none').mean([0, 1]).sum()\n","        reward_loss = 0.5 * F.mse_loss(predicted_rewards, rewards[:-1])\n","\n","        model_loss = kl_loss + obs_loss + reward_loss\n","        model_optimizer.zero_grad()\n","        model_loss.backward()\n","        clip_grad_norm_(model_params, clip_grad_norm)\n","        model_optimizer.step()\n","\n","        flatten_states = flatten_states.detach()\n","        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n","\n","        imagined_states = torch.zeros(imagination_horizon + 1,\n","                                         *flatten_states.shape,\n","                                          device=flatten_states.device)\n","        imagined_rnn_hiddens = torch.zeros(imagination_horizon + 1,\n","                                                *flatten_rnn_hiddens.shape,\n","                                                device=flatten_rnn_hiddens.device)\n","\n","        imagined_states[0] = flatten_states\n","        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n","\n","        for h in range(1, imagination_horizon + 1):\n","            actions = action_model(flatten_states, flatten_rnn_hiddens)\n","            flatten_states_prior, flatten_rnn_hiddens = rssm.transition.prior(rssm.transition.recurrent(flatten_states,\n","                                                                   actions,\n","                                                                   flatten_rnn_hiddens))\n","            flatten_states = flatten_states_prior.rsample()\n","            imagined_states[h] = flatten_states\n","            imagined_rnn_hiddens[h] = flatten_rnn_hiddens\n","\n","        flatten_imagined_states = imagined_states.view(-1, state_dim)\n","        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, rnn_hidden_dim)\n","        imagined_rewards = \\\n","            rssm.reward(flatten_imagined_states,\n","                        flatten_imagined_rnn_hiddens).view(imagination_horizon + 1, -1)\n","        imagined_values = \\\n","            value_model(flatten_imagined_states,\n","                        flatten_imagined_rnn_hiddens).view(imagination_horizon + 1, -1)\n","\n","        lambda_target_values = lambda_target(imagined_rewards, imagined_values, gamma, lambda_)\n","\n","        action_loss = -lambda_target_values.mean()\n","        action_optimizer.zero_grad()\n","        action_loss.backward()\n","        clip_grad_norm_(action_model.parameters(), clip_grad_norm)\n","        action_optimizer.step()\n","\n","        imagined_values = value_model(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(imagination_horizon + 1, -1)\n","        value_loss = 0.5 * F.mse_loss(imagined_values, lambda_target_values.detach())\n","        value_optimizer.zero_grad()\n","        value_loss.backward()\n","        clip_grad_norm_(value_model.parameters(), clip_grad_norm)\n","        value_optimizer.step()\n","\n","        print('update_step: %3d model loss: %.5f, kl_loss: %.5f, '\n","             'obs_loss: %.5f, reward_loss: %.5f, '\n","             'value_loss: %.5f action_loss: %.5f'\n","                % (update_step + 1, model_loss.item(), kl_loss.item(),\n","                    obs_loss.item(), reward_loss.item(),\n","                    value_loss.item(), action_loss.item()))\n","        total_update_step = episode * collect_interval + update_step\n","\n","    print('elasped time for update: %.2fs' % (time.time() - start))\n","\n","    if (episode + 1) % test_interval == 0:\n","        policy = Agent(encoder, rssm.transition, action_model)\n","        start = time.time()\n","        obs = env.reset()\n","        done = False\n","        total_reward = 0\n","        _returns = []\n","        while not done:\n","            action = policy(obs, training=False)\n","            obs, reward, done, _ = env.step(action)\n","            total_reward += reward\n","\n","        print('Total test reward at episode [%4d/%4d] is %f' %\n","                (episode+1, all_episodes, total_reward))\n","        print('elasped time for test: %.2fs' % (time.time() - start))\n","\n","    if (episode + 1) % model_save_interval == 0:\n","        model_log_dir = os.path.join(log_dir, 'episode_%04d' % (episode + 1))\n","        os.makedirs(model_log_dir)\n","        torch.save(encoder.state_dict(), os.path.join(model_log_dir, 'encoder.pth'))\n","        torch.save(rssm.transition.state_dict(), os.path.join(model_log_dir, 'rssm.pth'))\n","        torch.save(value_model.state_dict(), os.path.join(model_log_dir, 'value_model.pth'))\n","        torch.save(action_model.state_dict(), os.path.join(model_log_dir, 'action_model.pth'))\n","    del env\n","    gc.collect()\n","writer.close()"]},{"cell_type":"markdown","metadata":{"id":"E3F2tAU0kepm"},"source":["# 結果"]},{"cell_type":"code","source":["%tensorboard --logdir='./logs'"],"metadata":{"id":"6liscr55kCX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hp4rkT4UnGR6"},"outputs":[],"source":["env = humanoid_env()\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","encoder = Encoder().to(device)\n","rssm = RSSM(state_dim,env.action_space.shape[0],rnn_hidden_dim, )\n","value_model = ValueModel(state_dim, rnn_hidden_dim).to(device)\n","action_model = ActionModel(state_dim, rnn_hidden_dim,\n","                             env.action_space.shape[0]).to(device)\n","\n","torch.load(log_dir, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phTlvvC8GZdU"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from matplotlib import animation\n","from IPython.display import HTML\n","\n","\n","def display_video(frames):\n","    plt.figure(figsize=(8, 8), dpi=50)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","        plt.title(\"Step %d\" % (i))\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n","    display(HTML(anim.to_jshtml(default_mode='once')))\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Tsrk0bfEXpO"},"outputs":[],"source":["policy = Agent(encoder, rssm.transition, action_model)\n","\n","obs = env.reset()\n","done = False\n","total_reward = 0\n","frames = [obs]\n","actions = []\n","\n","while not done:\n","    action = policy(obs, training=False)\n","    obs, reward, done, _ = env.step(action)\n","\n","    total_reward += reward\n","    frames.append(obs)\n","    actions.append(action)\n","\n","print('Total Reward:', total_reward)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN-6pLFxG6sz"},"outputs":[],"source":["display_video(frames=frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G6gZU4iOSENK"},"outputs":[],"source":["actions = np.stack(actions)\n","np.save(\"actions\", actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mLx5YH7AcyYQ"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"1c906a337007ca492b40f9e66323e61f3dcaf71886120485625fb02da1be1aa9"}}},"nbformat":4,"nbformat_minor":0}