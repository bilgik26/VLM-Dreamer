{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDN8Ohc0Mh8C"
      },
      "source": [
        "# 環境構築"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "k-Tns5JvffDZ"
      },
      "outputs": [],
      "source": [
        "!tar -zxvf mujoco210-linux-x86_64.tar.gz\n",
        "!mkdir ~/.mujoco\n",
        "!cp -r mujoco210 ~/.mujoco/mujoco210\n",
        "!cp -r ~/.mujoco/mujoco210/bin/* /usr/lib/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49MbwGCWYpHB",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
        "!apt update && apt install libosmesa6-dev libgl1-mesa-glx libglfw3-dev patchelf xvfb freeglut3-dev libgles2-mesa-dev -y\n",
        "!pip3 install -U 'mujoco-py<2.2,>=2.1' pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HC1VmoSHffDc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = \"/root/.mujoco/mujoco210/bin\"\n",
        "os.environ[\"PYOPENGL_PLATFORM\"] = \"osmesa\"\n",
        "os.environ[\"MUJOCO_GL\"] = \"osmesa\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mujoco==2.3.5\n",
        "!pip install dm_control==1.0.9\n",
        "!pip install gym"
      ],
      "metadata": {
        "id": "bb3mf6vjacOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72zXgw6dMZ3_"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import glob\n",
        "import time\n",
        "from typing import Any, List, Tuple\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EApZNxvffDd"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "pydisplay = Display(visible=0, size=(400, 300))\n",
        "pydisplay.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pKzkQ_Jopg_"
      },
      "outputs": [],
      "source": [
        "env = gymnasium.make(\"Humanoid-v4\", render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "image = env.render()\n",
        "plt.imshow(image) # (480, 480, 3)\n",
        "plt.show()\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepMindControl:\n",
        "  metadata = {}\n",
        "\n",
        "  def __init__(self, name, size=(64, 64), camera=None):\n",
        "    domain, task = name.split('_', 1)\n",
        "    if domain == 'cup':  # Only domain with multiple words.\n",
        "      domain = 'ball_in_cup'\n",
        "    if isinstance(domain, str):\n",
        "      from dm_control import suite\n",
        "      self._env = suite.load(domain, task)\n",
        "    else:\n",
        "      assert task is None\n",
        "      self._env = domain()\n",
        "    self._size = size\n",
        "    if camera is None:\n",
        "      camera = dict(quadruped=2).get(domain, 0)\n",
        "    self._camera = camera\n",
        "\n",
        "  @property\n",
        "  def observation_space(self):\n",
        "    spaces = {}\n",
        "    for key, value in self._env.observation_spec().items():\n",
        "      spaces[key] = gym.spaces.Box(\n",
        "          -np.inf, np.inf, value.shape, dtype=np.float32)\n",
        "    spaces['image'] = gym.spaces.Box(\n",
        "        0, 255, self._size + (3,), dtype=np.uint8)\n",
        "    return gym.spaces.Dict(spaces)\n",
        "\n",
        "  @property\n",
        "  def action_space(self):\n",
        "    spec = self._env.action_spec()\n",
        "    return gym.spaces.Box(spec.minimum, spec.maximum, dtype=np.float32)\n",
        "\n",
        "  def step(self, action):\n",
        "    time_step = self._env.step(action)\n",
        "    obs = dict(time_step.observation)\n",
        "    obs['image'] = self.render()\n",
        "    reward = time_step.reward or 0\n",
        "    done = time_step.last()\n",
        "    info = {'discount': np.array(time_step.discount, np.float32)}\n",
        "    return obs['image'], reward, done, info\n",
        "\n",
        "  def reset(self):\n",
        "    time_step = self._env.reset()\n",
        "    obs = dict(time_step.observation)\n",
        "    obs['image'] = self.render()\n",
        "    return obs['image']\n",
        "\n",
        "  def render(self, *args, **kwargs):\n",
        "    if kwargs.get('mode', 'rgb_array') != 'rgb_array':\n",
        "      raise ValueError(\"Only render mode 'rgb_array' is supported.\")\n",
        "    return self._env.physics.render(*self._size, camera_id=self._camera)\n",
        "\n",
        "  def close(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "ZP5t3GMlbUEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RepeatAction(gym.Wrapper):\n",
        "    def __init__(self, env, skip=2):\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, info"
      ],
      "metadata": {
        "id": "Irq0yI10biQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwFmpA5qSb8I"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61uC5GNog_Iq"
      },
      "outputs": [],
      "source": [
        "env = DeepMindControl('walker_walk')\n",
        "env = RepeatAction(env)\n",
        "env.reset()\n",
        "obs, reward, done, info = env.step(env.action_space.sample())\n",
        "plt.imshow(obs) # (64, 64, 3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Model"
      ],
      "metadata": {
        "id": "WfLQwnmtclgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(num_groups=1, num_channels=dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)\n",
        "\n",
        "\n",
        "def Upsample(dim, dim_out=None):\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
        "        nn.Conv2d(dim, dim_out if exists(dim_out) else dim, kernel_size=3, padding=1),\n",
        "    )\n",
        "\n",
        "def Downsample(dim, dim_out=None):\n",
        "    return nn.Conv2d(\n",
        "        dim, dim_out if exists(dim_out) else dim, kernel_size=4, stride=2, padding=1\n",
        "    )"
      ],
      "metadata": {
        "id": "njHUfW2Dco5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionEncoding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        half_dim = self.dim // 2\n",
        "\n",
        "        pos_enc = math.log(10000) / (half_dim - 1)  # scalar\n",
        "        pos_enc = torch.exp(torch.arange(half_dim, device=time.device) * -pos_enc)  # ( time_emb_dim // 2, )\n",
        "        pos_enc = time[:, None] * pos_enc[None, :]  # ( batch, 1 ) * ( 1, time_emb_dim // 2 ) -> ( batch, time_emb_dim // 2 )\n",
        "        pos_enc = torch.cat((pos_enc.sin(), pos_enc.cos()), dim=-1)  # ( batch, time_emb_dim )\n",
        "\n",
        "        return pos_enc"
      ],
      "metadata": {
        "id": "7bLqfyufcxup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups=8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x ( b, dim, h', w' )\n",
        "        Returns:\n",
        "            x ( b, dim_out, h', w' )\n",
        "        \"\"\"\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        return self.act(x)\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(time_emb_dim, dim_out * 2)\n",
        "        ) if exists(time_emb_dim) else None\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x ( b, dim, h', w' )\n",
        "        Returns:\n",
        "            x ( b, dim_out, h', w' )\n",
        "        \"\"\"\n",
        "        scale_shift = None\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)  # ( b, dim_out * 2 )\n",
        "            time_emb = rearrange(time_emb, \"b c -> b c 1 1\")  # ( b, dim_out * 2, 1, 1 )\n",
        "            scale_shift = time_emb.chunk(2, dim=1)  # tuple(( b, dim_out, 1, 1 ), ( b, dim_out, 1, 1 ))\n",
        "\n",
        "        h = self.block1(x, scale_shift=scale_shift)\n",
        "        h = self.block2(h)\n",
        "\n",
        "        return h + self.res_conv(x)"
      ],
      "metadata": {
        "id": "_0NWVDCoc9qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32, num_mem_kv=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, dim, 1),\n",
        "            nn.GroupNorm(1, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x ( b, dim, h', w' )\n",
        "        Returns:\n",
        "            out ( b, dim, h', w' )\n",
        "        \"\"\"\n",
        "        b, c, h, w = x.shape\n",
        "\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q = rearrange(q, \"b (h c) x y -> b h c (x y)\", h=self.heads)\n",
        "        k = rearrange(k, \"b (h c) x y -> b h c (x y)\", h=self.heads)\n",
        "        v = rearrange(v, \"b (h c) x y -> b h c (x y)\", h=self.heads)\n",
        "        # ( b, heads, dim_head, h(=x) * w(=y) )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "\n",
        "        return self.to_out(out)"
      ],
      "metadata": {
        "id": "W4hXSiCjdbpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim, # image_size\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        with_time_emb=True,\n",
        "        resnet_block_groups=8,\n",
        "        convnext_mult=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # determine dimensions\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = init_dim if exists(init_dim) else dim // 3 * 2\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, kernel_size=7, padding=3)\n",
        "\n",
        "        # Wide ResNet\n",
        "        ResnetBlock_ = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # position (time) encoding\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEncoding(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(nn.ModuleList(\n",
        "                    [\n",
        "                        ResnetBlock_(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        ResnetBlock_(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding=1),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = ResnetBlock_(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, LinearAttention(mid_dim)))\n",
        "        self.mid_block2 = ResnetBlock_(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "            is_last = ind == (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(nn.ModuleList(\n",
        "                    [\n",
        "                        ResnetBlock_(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        ResnetBlock_(dim_out + dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Upsample(dim_out, dim_in) if not is_last else nn.Conv2d(dim_out, dim_in, 3, padding=1),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        out_dim = out_dim if exists(out_dim) else channels\n",
        "        self.final_res_block = ResnetBlock_(init_dim * 2, init_dim, time_emb_dim=time_dim)\n",
        "        self.final_conv = nn.Conv2d(init_dim, out_dim, 1)\n",
        "\n",
        "    def forward(self, x, time):  # time: ( b, )\n",
        "        b = x.shape[0]\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "        r = x.clone()\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None # ( B, time_emb_dim )\n",
        "\n",
        "        h = []\n",
        "\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            h.append(x)\n",
        "\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "\n",
        "            x = downsample(x)\n",
        "\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "\n",
        "            x = upsample(x)\n",
        "\n",
        "        x = torch.cat((x, r), dim=1)\n",
        "\n",
        "        x = self.final_res_block(x, t)\n",
        "\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "pewSlL9Qde2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = 800\n",
        "\n",
        "def linear_beta_schedule(timesteps, beta_start=1e-4, beta_end=0.02):\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "betas = linear_beta_schedule(timesteps)\n",
        "\n",
        "alphas_cumprod = torch.cumprod(1. - betas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)"
      ],
      "metadata": {
        "id": "5JxKpsNLfRrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(a, t, x_shape: torch.Size):\n",
        "    batch_size = t.shape[0]\n",
        "\n",
        "    out = a.gather(-1, t.to(a.device))\n",
        "\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "def q_sample(x_start, t, noise):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x_start ( b, c, h, w )\n",
        "        t ( b, )\n",
        "        noise ( b, c, h, w )\n",
        "    Returns:\n",
        "        x_noisy: ( b, c, h, w )\n",
        "    \"\"\"\n",
        "    alphas_cumprod_t = extract(alphas_cumprod, t, x_start.shape) # ( b, 1, 1, 1 )\n",
        "\n",
        "    return torch.sqrt(alphas_cumprod_t) * x_start + torch.sqrt(1.0 - alphas_cumprod_t) * noise"
      ],
      "metadata": {
        "id": "1JRqdCFSfYl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l2\"):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x_start ( b, c, h, w )\n",
        "        t ( b, )\n",
        "    \"\"\"\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start, t, noise)\n",
        "\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        # https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "htnpVCk_gtTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculations for posterior q(x_{t-1} | x_t, x_0)\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(model, x, t_index):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x ( b, c, h, w )\n",
        "        t_index ( int )\n",
        "    Returns:\n",
        "        x ( b, c, h, w )\n",
        "    \"\"\"\n",
        "    t = torch.full((x.shape[0],), t_index, device=x.device, dtype=torch.long) # ( b, )\n",
        "\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(torch.sqrt(1.0 - alphas_cumprod), t, x.shape)\n",
        "    coef_t = extract(torch.sqrt(1.0 / (1. - betas)), t, x.shape)\n",
        "\n",
        "    model_mean = coef_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, image_size, batch_size=16, channels=3):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        image_size ( int )\n",
        "    Returns:\n",
        "        imgs ( b, c, h, w )\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    shape = (batch_size, channels, image_size, image_size)\n",
        "    img = torch.randn(shape, device=device)\n",
        "\n",
        "    imgs = []\n",
        "    for t in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, t)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "\n",
        "    return imgs"
      ],
      "metadata": {
        "id": "8bclwH7Pg8BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3rWEGtkA3e_"
      },
      "source": [
        "# Dreamer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUt9Lp3HMeJq"
      },
      "outputs": [],
      "source": [
        "class TransitionModel(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, rnn_hidden_dim,\n",
        "                 hidden_dim=200, min_stddev=0.1, act=F.elu):\n",
        "        super(TransitionModel, self).__init__()\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.rnn_hidden_dim = rnn_hidden_dim\n",
        "        self.fc_state_action = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "\n",
        "        self.fc_rnn_hidden = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc_state_mean_prior = nn.Linear(hidden_dim, state_dim)\n",
        "        self.fc_state_stddev_prior = nn.Linear(hidden_dim, state_dim)\n",
        "        self.fc_rnn_hidden_embedded_obs = nn.Linear(rnn_hidden_dim + 1024, hidden_dim)\n",
        "        self.fc_state_mean_posterior = nn.Linear(hidden_dim, state_dim)\n",
        "        self.fc_state_stddev_posterior = nn.Linear(hidden_dim, state_dim)\n",
        "\n",
        "        self.rnn = nn.GRUCell(hidden_dim, rnn_hidden_dim)\n",
        "        self._min_stddev = min_stddev\n",
        "        self.act = act\n",
        "\n",
        "\n",
        "    def forward(self, state, action, rnn_hidden, embedded_next_obs):\n",
        "        next_state_prior, rnn_hidden = self.prior(self.recurrent(state, action, rnn_hidden))\n",
        "        next_state_posterior = self.posterior(rnn_hidden, embedded_next_obs)\n",
        "        return next_state_prior, next_state_posterior, rnn_hidden\n",
        "\n",
        "    def recurrent(self, state, action, rnn_hidden):\n",
        "        \"\"\"\n",
        "        h_t+1 = f(h_t, s_t, a_t)\n",
        "        \"\"\"\n",
        "        hidden = self.act(self.fc_state_action(torch.cat([state, action], dim=1)))\n",
        "        rnn_hidden = self.rnn(hidden, rnn_hidden)\n",
        "        return rnn_hidden\n",
        "\n",
        "    def prior(self, rnn_hidden):\n",
        "        \"\"\"\n",
        "        prior p(s_t+1 | h_t+1)\n",
        "        \"\"\"\n",
        "        hidden = self.act(self.fc_rnn_hidden(rnn_hidden))\n",
        "\n",
        "        mean = self.fc_state_mean_prior(hidden)\n",
        "        stddev = F.softplus(self.fc_state_stddev_prior(hidden)) + self._min_stddev\n",
        "        return Normal(mean, stddev), rnn_hidden\n",
        "\n",
        "    def posterior(self, rnn_hidden, embedded_obs):\n",
        "        \"\"\"\n",
        "        posterior q(s_t+1 | h_t+1, e_t+1)\n",
        "        \"\"\"\n",
        "        hidden = self.act(\n",
        "            self.fc_rnn_hidden_embedded_obs(\n",
        "                torch.cat([rnn_hidden, embedded_obs], dim=1)\n",
        "            )\n",
        "        )\n",
        "        mean = self.fc_state_mean_posterior(hidden)\n",
        "        stddev = F.softplus(self.fc_state_stddev_posterior(hidden)) + self._min_stddev\n",
        "        return Normal(mean, stddev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmsUzBFYMeJq"
      },
      "outputs": [],
      "source": [
        "class ObservationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    p(o_t | s_t, h_t)\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, rnn_hidden_dim):\n",
        "        super(ObservationModel, self).__init__()\n",
        "        self.fc = nn.Linear(state_dim + rnn_hidden_dim, 1024)\n",
        "        self.dc1 = nn.ConvTranspose2d(1024, 128, kernel_size=5, stride=2)\n",
        "        self.dc2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2)\n",
        "        self.dc3 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2)\n",
        "        self.dc4 = nn.ConvTranspose2d(32, 3, kernel_size=6, stride=2)\n",
        "\n",
        "\n",
        "    def forward(self, state, rnn_hidden):\n",
        "        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n",
        "        hidden = hidden.view(hidden.size(0), 1024, 1, 1)\n",
        "        hidden = F.relu(self.dc1(hidden))\n",
        "        hidden = F.relu(self.dc2(hidden))\n",
        "        hidden = F.relu(self.dc3(hidden))\n",
        "        obs = self.dc4(hidden)\n",
        "        return obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c0l4yGSo9Fk"
      },
      "outputs": [],
      "source": [
        "class RewardModel(nn.Module):\n",
        "    \"\"\"\n",
        "    p(r_t | s_t, h_t)\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim, rnn_hidden_dim, hidden_dim=400, act=F.elu):\n",
        "        super(RewardModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "        self.act = act\n",
        "\n",
        "\n",
        "    def forward(self, state, rnn_hidden):\n",
        "        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = self.act(self.fc2(hidden))\n",
        "        hidden = self.act(self.fc3(hidden))\n",
        "        reward = self.fc4(hidden)\n",
        "        return reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzJB9ZiOxMnn"
      },
      "outputs": [],
      "source": [
        "class RSSM(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, rnn_hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.transition = TransitionModel(state_dim, action_dim, rnn_hidden_dim).to(device)\n",
        "        self.observation = ObservationModel(state_dim, rnn_hidden_dim,).to(device)\n",
        "        self.reward = RewardModel(state_dim, rnn_hidden_dim,).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmptCR67aYZc"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity, observation_shape, action_dim):\n",
        "        self.capacity = capacity\n",
        "\n",
        "        self.observations = np.zeros((capacity, *observation_shape), dtype=np.uint8)\n",
        "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
        "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
        "        self.done = np.zeros((capacity, 1), dtype=np.bool_)\n",
        "\n",
        "        self.index = 0\n",
        "        self.is_filled = False\n",
        "\n",
        "    def push(self, observation, action, reward, done):\n",
        "        self.observations[self.index] = observation\n",
        "        self.actions[self.index] = action\n",
        "        self.rewards[self.index] = reward\n",
        "        self.done[self.index] = done\n",
        "\n",
        "        if self.index == self.capacity - 1:\n",
        "            self.is_filled = True\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, chunk_length):\n",
        "        episode_borders = np.where(self.done)[0]\n",
        "        sampled_indexes = []\n",
        "        for _ in range(batch_size):\n",
        "            cross_border = True\n",
        "            while cross_border:\n",
        "                initial_index = np.random.randint(len(self) - chunk_length + 1)\n",
        "                final_index = initial_index + chunk_length - 1\n",
        "                cross_border = np.logical_and(initial_index <= episode_borders,\n",
        "                                              episode_borders < final_index).any()\n",
        "            sampled_indexes += list(range(initial_index, final_index + 1))\n",
        "\n",
        "        sampled_observations = self.observations[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, *self.observations.shape[1:])\n",
        "        sampled_actions = self.actions[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, self.actions.shape[1])\n",
        "        sampled_rewards = self.rewards[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        sampled_done = self.done[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.is_filled else self.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLIt0qiya1_2"
      },
      "outputs": [],
      "source": [
        "def preprocess_obs(obs):\n",
        "    \"\"\"\n",
        "    [0, 255] -> [-0.5, 0.5]\n",
        "    \"\"\"\n",
        "    obs = obs.astype(np.float32)\n",
        "    normalized_obs = obs / 255.0 - 0.5\n",
        "    return normalized_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLZMKJj_bIHr"
      },
      "outputs": [],
      "source": [
        "def lambda_target(rewards, values, gamma, lambda_):\n",
        "    \"\"\"\n",
        "    λ-return\n",
        "    \"\"\"\n",
        "    V_lambda = torch.zeros_like(rewards, device=rewards.device)\n",
        "\n",
        "    H = rewards.shape[0] - 1\n",
        "    V_n = torch.zeros_like(rewards, device=rewards.device)\n",
        "    V_n[H] = values[H]\n",
        "    for n in range(1, H+1):\n",
        "        V_n[:-n] = (gamma ** n) * values[n:]\n",
        "        for k in range(1, n+1):\n",
        "            if k == n:\n",
        "                V_n[:-n] += (gamma ** (n-1)) * rewards[k:]\n",
        "            else:\n",
        "                V_n[:-n] += (gamma ** (k-1)) * rewards[k:-n+k]\n",
        "\n",
        "        if n == H:\n",
        "            V_lambda += (lambda_ ** (H-1)) * V_n\n",
        "        else:\n",
        "            V_lambda += (1 - lambda_) * (lambda_ ** (n-1)) * V_n\n",
        "\n",
        "    return V_lambda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGY-YZzmSh-3"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.cv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2)\n",
        "        self.cv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.cv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2)\n",
        "        self.cv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        hidden = F.relu(self.cv1(obs))\n",
        "        hidden = F.relu(self.cv2(hidden))\n",
        "        hidden = F.relu(self.cv3(hidden))\n",
        "        embedded_obs = F.relu(self.cv4(hidden)).reshape(hidden.size(0), -1)\n",
        "        return embedded_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gegVfLVWLLR"
      },
      "outputs": [],
      "source": [
        "class ValueModel(nn.Module):\n",
        "    def __init__(self, state_dim, rnn_hidden_dim, hidden_dim=400, act=F.elu):\n",
        "        super(ValueModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, state, rnn_hidden):\n",
        "        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = self.act(self.fc2(hidden))\n",
        "        hidden = self.act(self.fc3(hidden))\n",
        "        state_value = self.fc4(hidden)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2iSa_-kW_W7"
      },
      "outputs": [],
      "source": [
        "class ActionModel(nn.Module):\n",
        "    def __init__(self, state_dim, rnn_hidden_dim, action_dim,\n",
        "                 hidden_dim=400, act=F.elu, min_stddev=1e-4, init_stddev=5.0):\n",
        "        super(ActionModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc_mean = nn.Linear(hidden_dim, action_dim)\n",
        "        self.fc_stddev = nn.Linear(hidden_dim, action_dim)\n",
        "        self.act = act\n",
        "        self.min_stddev = min_stddev\n",
        "        self.init_stddev = np.log(np.exp(init_stddev) - 1)\n",
        "\n",
        "    def forward(self, state, rnn_hidden, training=True):\n",
        "        hidden = self.act(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = self.act(self.fc2(hidden))\n",
        "        hidden = self.act(self.fc3(hidden))\n",
        "        hidden = self.act(self.fc4(hidden))\n",
        "\n",
        "        mean = self.fc_mean(hidden)\n",
        "        mean = 5.0 * torch.tanh(mean / 5.0)\n",
        "        stddev = self.fc_stddev(hidden)\n",
        "        stddev = F.softplus(stddev + self.init_stddev) + self.min_stddev\n",
        "\n",
        "        if training:\n",
        "            action = torch.tanh(Normal(mean, stddev).rsample())\n",
        "        else:\n",
        "            action = torch.tanh(mean)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dof4rokgYj-6"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    def __init__(self, encoder, rssm, action_model):\n",
        "        self.encoder = encoder\n",
        "        self.rssm = rssm\n",
        "        self.action_model = action_model\n",
        "\n",
        "        self.device = next(self.action_model.parameters()).device\n",
        "        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n",
        "\n",
        "    def __call__(self, obs, training=True):\n",
        "        obs = preprocess_obs(obs)\n",
        "        obs = torch.as_tensor(obs, device=self.device)\n",
        "        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedded_obs = self.encoder(obs)\n",
        "            state_posterior = self.rssm.posterior(self.rnn_hidden, embedded_obs)\n",
        "            state = state_posterior.sample()\n",
        "            action = self.action_model(state, self.rnn_hidden, training=training)\n",
        "            _, self.rnn_hidden = self.rssm.prior(self.rssm.recurrent(state, action, self.rnn_hidden))\n",
        "\n",
        "        return action.squeeze().cpu().numpy()\n",
        "\n",
        "    def reset(self):\n",
        "        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUPWAkpyc9Z-"
      },
      "source": [
        "# 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8sW8K5Kdjrg"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "env = humanoid_env()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "buffer_capacity = 200000\n",
        "replay_buffer = ReplayBuffer(\n",
        "    capacity=buffer_capacity,\n",
        "    observation_shape=env.observation_space.shape,\n",
        "    action_dim=env.action_space.shape[0]\n",
        ")\n",
        "\n",
        "state_dim = 30\n",
        "rnn_hidden_dim = 200\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "rssm = RSSM(state_dim,env.action_space.shape[0],rnn_hidden_dim, )\n",
        "value_model = ValueModel(state_dim, rnn_hidden_dim).to(device)\n",
        "action_model = ActionModel(state_dim, rnn_hidden_dim,\n",
        "                             env.action_space.shape[0]).to(device)\n",
        "\n",
        "trained_models = TrainedModels(\n",
        "    encoder, rssm, value_model, action_model\n",
        ")\n",
        "\n",
        "\n",
        "model_lr = 6e-4\n",
        "value_lr = 8e-5\n",
        "action_lr = 8e-5\n",
        "eps = 1e-4\n",
        "model_params = (list(encoder.parameters()) +\n",
        "                  list(rssm.transition.parameters()) +\n",
        "                  list(rssm.observation.parameters()) +\n",
        "                  list(rssm.reward.parameters()))\n",
        "model_optimizer = torch.optim.Adam(model_params, lr=model_lr, eps=eps)\n",
        "value_optimizer = torch.optim.Adam(value_model.parameters(), lr=value_lr, eps=eps)\n",
        "action_optimizer = torch.optim.Adam(action_model.parameters(), lr=action_lr, eps=eps)\n",
        "\n",
        "\n",
        "seed_episodes = 2 # 5\n",
        "all_episodes = 10 # 100\n",
        "test_interval = 2 # 10\n",
        "model_save_interval = 4 # 20\n",
        "collect_interval = 10 # 100\n",
        "\n",
        "action_noise_var = 0.3\n",
        "\n",
        "batch_size = 50\n",
        "chunk_length = 50\n",
        "imagination_horizon = 15\n",
        "\n",
        "gamma = 0.9\n",
        "lambda_ = 0.95\n",
        "clip_grad_norm = 100\n",
        "free_nats = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txKI9CCne5e3"
      },
      "outputs": [],
      "source": [
        "for episode in range(seed_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        next_obs, reward, done, _= env.step(action)\n",
        "        replay_buffer.push(obs, action, reward, done)\n",
        "        obs = next_obs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"logs\"\n",
        "writer = SummaryWriter(log_dir)"
      ],
      "metadata": {
        "id": "YvYgvAE-jXHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_LNryZQfnKT"
      },
      "outputs": [],
      "source": [
        "for episode in range(seed_episodes, all_episodes):\n",
        "    start = time.time()\n",
        "    policy = Agent(encoder, rssm.transition, action_model)\n",
        "\n",
        "    env = CLIPRewardedHumanoidEnv()\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        action = policy(obs)\n",
        "        action += np.random.normal(0, np.sqrt(action_noise_var),\n",
        "                                     env.action_space.shape[0])\n",
        "        next_obs, reward, done, _, = env.step(action)\n",
        "\n",
        "        replay_buffer.push(obs, action, reward, done)\n",
        "\n",
        "        obs = next_obs\n",
        "        total_reward += reward\n",
        "\n",
        "    print('episode [%4d/%4d] is collected. Total reward is %f' %\n",
        "            (episode+1, all_episodes, total_reward))\n",
        "    print('elasped time for interaction: %.2fs' % (time.time() - start))\n",
        "\n",
        "    start = time.time()\n",
        "    for update_step in range(collect_interval):\n",
        "        observations, actions, rewards, _ = \\\n",
        "            replay_buffer.sample(batch_size, chunk_length)\n",
        "\n",
        "        observations = preprocess_obs(observations)\n",
        "        observations = torch.as_tensor(observations, device=device)\n",
        "        observations = observations.transpose(3, 4).transpose(2, 3)\n",
        "        observations = observations.transpose(0, 1)\n",
        "        actions = torch.as_tensor(actions, device=device).transpose(0, 1)\n",
        "        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)\n",
        "\n",
        "        embedded_observations = encoder(\n",
        "            observations.reshape(-1, 3, 64, 64)).view(chunk_length, batch_size, -1)\n",
        "\n",
        "        states = torch.zeros(chunk_length, batch_size, state_dim, device=device)\n",
        "        rnn_hiddens = torch.zeros(chunk_length, batch_size, rnn_hidden_dim, device=device)\n",
        "\n",
        "        state = torch.zeros(batch_size, state_dim, device=device)\n",
        "        rnn_hidden = torch.zeros(batch_size, rnn_hidden_dim, device=device)\n",
        "\n",
        "        kl_loss = 0\n",
        "        for l in range(chunk_length-1):\n",
        "            next_state_prior, next_state_posterior, rnn_hidden = \\\n",
        "                rssm.transition(state, actions[l], rnn_hidden, embedded_observations[l+1])\n",
        "            state = next_state_posterior.rsample()\n",
        "            states[l+1] = state\n",
        "            rnn_hiddens[l+1] = rnn_hidden\n",
        "            kl = kl_divergence(next_state_prior, next_state_posterior).sum(dim=1)\n",
        "            kl_loss += kl.clamp(min=free_nats).mean()\n",
        "        kl_loss /= (chunk_length - 1)\n",
        "\n",
        "        states = states[1:]\n",
        "        rnn_hiddens = rnn_hiddens[1:]\n",
        "\n",
        "        flatten_states = states.view(-1, state_dim)\n",
        "        flatten_rnn_hiddens = rnn_hiddens.view(-1, rnn_hidden_dim)\n",
        "        recon_observations = rssm.observation(flatten_states, flatten_rnn_hiddens).view(chunk_length-1, batch_size, 3, 64, 64)\n",
        "        predicted_rewards = rssm.reward(flatten_states, flatten_rnn_hiddens).view(chunk_length-1, batch_size, 1)\n",
        "\n",
        "        obs_loss = 0.5 * F.mse_loss(recon_observations, observations[1:], reduction='none').mean([0, 1]).sum()\n",
        "        reward_loss = 0.5 * F.mse_loss(predicted_rewards, rewards[:-1])\n",
        "\n",
        "        model_loss = kl_loss + obs_loss + reward_loss\n",
        "        model_optimizer.zero_grad()\n",
        "        model_loss.backward()\n",
        "        clip_grad_norm_(model_params, clip_grad_norm)\n",
        "        model_optimizer.step()\n",
        "\n",
        "        flatten_states = flatten_states.detach()\n",
        "        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n",
        "\n",
        "        imagined_states = torch.zeros(imagination_horizon + 1,\n",
        "                                         *flatten_states.shape,\n",
        "                                          device=flatten_states.device)\n",
        "        imagined_rnn_hiddens = torch.zeros(imagination_horizon + 1,\n",
        "                                                *flatten_rnn_hiddens.shape,\n",
        "                                                device=flatten_rnn_hiddens.device)\n",
        "\n",
        "        imagined_states[0] = flatten_states\n",
        "        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n",
        "\n",
        "        for h in range(1, imagination_horizon + 1):\n",
        "            actions = action_model(flatten_states, flatten_rnn_hiddens)\n",
        "            flatten_states_prior, flatten_rnn_hiddens = rssm.transition.prior(rssm.transition.recurrent(flatten_states,\n",
        "                                                                   actions,\n",
        "                                                                   flatten_rnn_hiddens))\n",
        "            flatten_states = flatten_states_prior.rsample()\n",
        "            imagined_states[h] = flatten_states\n",
        "            imagined_rnn_hiddens[h] = flatten_rnn_hiddens\n",
        "\n",
        "        flatten_imagined_states = imagined_states.view(-1, state_dim)\n",
        "        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, rnn_hidden_dim)\n",
        "        imagined_rewards = \\\n",
        "            rssm.reward(flatten_imagined_states,\n",
        "                        flatten_imagined_rnn_hiddens).view(imagination_horizon + 1, -1)\n",
        "        imagined_values = \\\n",
        "            value_model(flatten_imagined_states,\n",
        "                        flatten_imagined_rnn_hiddens).view(imagination_horizon + 1, -1)\n",
        "\n",
        "        lambda_target_values = lambda_target(imagined_rewards, imagined_values, gamma, lambda_)\n",
        "\n",
        "        action_loss = -lambda_target_values.mean()\n",
        "        action_optimizer.zero_grad()\n",
        "        action_loss.backward()\n",
        "        clip_grad_norm_(action_model.parameters(), clip_grad_norm)\n",
        "        action_optimizer.step()\n",
        "\n",
        "        imagined_values = value_model(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(imagination_horizon + 1, -1)\n",
        "        value_loss = 0.5 * F.mse_loss(imagined_values, lambda_target_values.detach())\n",
        "        value_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        clip_grad_norm_(value_model.parameters(), clip_grad_norm)\n",
        "        value_optimizer.step()\n",
        "\n",
        "        print('update_step: %3d model loss: %.5f, kl_loss: %.5f, '\n",
        "             'obs_loss: %.5f, reward_loss: %.5f, '\n",
        "             'value_loss: %.5f action_loss: %.5f'\n",
        "                % (update_step + 1, model_loss.item(), kl_loss.item(),\n",
        "                    obs_loss.item(), reward_loss.item(),\n",
        "                    value_loss.item(), action_loss.item()))\n",
        "        total_update_step = episode * collect_interval + update_step\n",
        "\n",
        "    print('elasped time for update: %.2fs' % (time.time() - start))\n",
        "\n",
        "    if (episode + 1) % test_interval == 0:\n",
        "        policy = Agent(encoder, rssm.transition, action_model)\n",
        "        start = time.time()\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        _returns = []\n",
        "        while not done:\n",
        "            action = policy(obs, training=False)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "        print('Total test reward at episode [%4d/%4d] is %f' %\n",
        "                (episode+1, all_episodes, total_reward))\n",
        "        print('elasped time for test: %.2fs' % (time.time() - start))\n",
        "\n",
        "    if (episode + 1) % model_save_interval == 0:\n",
        "        model_log_dir = os.path.join(log_dir, 'episode_%04d' % (episode + 1))\n",
        "        os.makedirs(model_log_dir)\n",
        "        torch.save(encoder.state_dict(), os.path.join(model_log_dir, 'encoder.pth'))\n",
        "        torch.save(rssm.transition.state_dict(), os.path.join(model_log_dir, 'rssm.pth'))\n",
        "        torch.save(value_model.state_dict(), os.path.join(model_log_dir, 'value_model.pth'))\n",
        "        torch.save(action_model.state_dict(), os.path.join(model_log_dir, 'action_model.pth'))\n",
        "    del env\n",
        "    gc.collect()\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3F2tAU0kepm"
      },
      "source": [
        "# 結果"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir='./logs'"
      ],
      "metadata": {
        "id": "sDu6HWvSkLF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp4rkT4UnGR6"
      },
      "outputs": [],
      "source": [
        "env = humanoid_env()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "rssm = RSSM(state_dim,env.action_space.shape[0],rnn_hidden_dim, )\n",
        "value_model = ValueModel(state_dim, rnn_hidden_dim).to(device)\n",
        "action_model = ActionModel(state_dim, rnn_hidden_dim,\n",
        "                             env.action_space.shape[0]).to(device)\n",
        "\n",
        "torch.load(log_dir, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phTlvvC8GZdU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "\n",
        "def display_video(frames):\n",
        "    plt.figure(figsize=(8, 8), dpi=50)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "        plt.title(\"Step %d\" % (i))\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    display(HTML(anim.to_jshtml(default_mode='once')))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Tsrk0bfEXpO"
      },
      "outputs": [],
      "source": [
        "policy = Agent(encoder, rssm.transition, action_model)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = [obs]\n",
        "actions = []\n",
        "\n",
        "while not done:\n",
        "    action = policy(obs, training=False)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    total_reward += reward\n",
        "    frames.append(obs)\n",
        "    actions.append(action)\n",
        "\n",
        "print('Total Reward:', total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN-6pLFxG6sz"
      },
      "outputs": [],
      "source": [
        "display_video(frames=frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6gZU4iOSENK"
      },
      "outputs": [],
      "source": [
        "actions = np.stack(actions)\n",
        "np.save(\"actions\", actions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLx5YH7AcyYQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "1c906a337007ca492b40f9e66323e61f3dcaf71886120485625fb02da1be1aa9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}